{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "d_x = 1\n",
    "d_y = 1\n",
    "\n",
    "target_stock = 'CSCO'\n",
    "stock_list = [target_stock]\n",
    "n_stocks = 1\n",
    "\n",
    "start_date = '20150101'\n",
    "end_date = '20151231'\n",
    "min_day_obs = 252\n",
    "\n",
    "sampling_freq = '100ms'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data from s3 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket = 'tradelake'\n",
    "key = 'sp500_2015/%s_%s.csv.gz'\n",
    "colnames = ['date',\n",
    "            'time',\n",
    "            'ex',\n",
    "            'symbol',\n",
    "            'trade_cond',\n",
    "            'size',\n",
    "            'price',\n",
    "            'stopinf',\n",
    "            'corr',\n",
    "            'seqnum',\n",
    "            'source',\n",
    "            'rf']\n",
    "\n",
    "def check_key_in_bucket(bucket_name, key):\n",
    "    try:\n",
    "        with open('/root/start.sh', 'rb') as input_file:\n",
    "            AWSAccessKeyId = input_file.readline()[:-1]\n",
    "            AWSSecretKey = input_file.readline()[:-1]    \n",
    "        s3 = boto3.client('s3', \n",
    "                          aws_access_key_id=AWSAccessKeyId, \n",
    "                          aws_secret_access_key=AWSSecretKey)\n",
    "        s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    except Exception as e:\n",
    "        exists = False\n",
    "    else:\n",
    "        exists = True\n",
    "    return exists\n",
    "\n",
    "def read_csv_from_s3(bucket, key, colnames, parse_dates, index_to_set):\n",
    "    \n",
    "    with open('/root/start.sh', 'rb') as input_file:\n",
    "        AWSAccessKeyId = input_file.readline()[:-1]\n",
    "        AWSSecretKey = input_file.readline()[:-1]\n",
    "\n",
    "    print 'Reading csv from bucket %s key %s' % (bucket, key)\n",
    "\n",
    "    local_file = key.split('/')[-1]\n",
    "    \n",
    "    s3 = boto3.client('s3', \n",
    "                      aws_access_key_id=AWSAccessKeyId, \n",
    "                      aws_secret_access_key=AWSSecretKey)\n",
    "\n",
    "    s3.download_file(bucket, key, local_file)\n",
    "    \n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(\n",
    "        local_file, \n",
    "        names=colnames,\n",
    "        parse_dates=parse_dates, \n",
    "        compression='gzip',\n",
    "        engine='c').set_index(index_to_set)\n",
    "\n",
    "    print 'Read csv from bucket %s key %s' % (bucket, key)\n",
    "    \n",
    "    os.remove(local_file)\n",
    "\n",
    "    return df[['price', 'size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "    \n",
    "dates = pd.date_range(start_date, end_date).map(lambda x: ''.join(str(x).split(' ')[0].split('-')))\n",
    "\n",
    "dates = [date for date in dates if check_key_in_bucket(bucket, key % (date, stock_name))]\n",
    "\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers_trades(time_series, window=1000):\n",
    "    time_series['size_upper_bound'] = pd.rolling_quantile(time_series['size'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.99,\n",
    "                                                     min_periods=1)\n",
    "    time_series['size_lower_bound'] = pd.rolling_quantile(time_series['size'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.01,\n",
    "                                                     min_periods=1)\n",
    "    time_series = time_series[time_series['size'] < time_series['size_upper_bound']]\n",
    "    time_series = time_series[time_series['size'] > time_series['size_lower_bound']]\n",
    "\n",
    "    time_series.drop(['size_upper_bound', 'size_lower_bound'], axis = 1)\n",
    "\n",
    "    time_series['price_upper_bound'] = pd.rolling_quantile(time_series['price'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.99,\n",
    "                                                     min_periods=1)\n",
    "    time_series['price_lower_bound'] = pd.rolling_quantile(time_series['price'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.01,\n",
    "                                                     min_periods=1)\n",
    "    time_series = time_series[time_series['price'] < time_series['price_upper_bound']]\n",
    "    time_series = time_series[time_series['price'] > time_series['price_lower_bound']]\n",
    "\n",
    "    time_series.drop(['price_upper_bound', 'price_lower_bound'], axis = 1)\n",
    "\n",
    "    price_median = time_series['price'].median()\n",
    "\n",
    "    time_series = time_series[time_series['price'] > price_median * 0.80]\n",
    "    time_series = time_series[time_series['price'] < price_median * 1.20]\n",
    "\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resample_trades(key, time_series):\n",
    "    \n",
    "    key_date = key[11:].split('_')[0]    \n",
    "    \n",
    "    start_timestamp = pd.to_datetime(key_date + 'T10:00:00')\n",
    "    end_timestamp = pd.to_datetime(key_date + 'T14:00:00')\n",
    "        \n",
    "    time_series = time_series.resample(sampling_freq).median().fillna(method='pad')    \n",
    "    complete_result = time_series[start_timestamp:end_timestamp]['price'].diff()[1:]\n",
    "    \n",
    "    n_samples = len(complete_result)\n",
    "    \n",
    "    return [complete_result[offset : offset + split_window].values\n",
    "            for offset in xrange(0, n_samples, split_window)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_names = [stock_name for stock_name in stock_list[:n_stocks]\n",
    "               if stock_name not in exclusion_list]\n",
    "\n",
    "key_list = [key % (date, stock_name)\n",
    "            for stock_name in stock_list[:n_stocks]\n",
    "            if stock_name not in exclusion_list\n",
    "            for date in dates[:n_days]\n",
    "            if date not in excluded_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = [read_csv_from_s3(bucket, k, None,\n",
    "                             {'datetime': ['date', 'time']},\n",
    "                             'datetime')\n",
    "            for k in key_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We do not assume we know the sequence length yet\n",
    "\n",
    "x_seq_ph = tf.placeholder(shape=(batch_size, None, d_x), dtype=tf.float32)\n",
    "y_seq_ph = tf.placeholder(shape=(batch_size, None, d_y), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_layer(input_seq, n_dims_in, n_dims_out, width, \n",
    "                      dilation=1, causal=True):\n",
    "    conv_kernel = tf.get_variable(\n",
    "        name=\"kernel\",\n",
    "        shape=[width, n_dims_in, n_dims_out], \n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    # Similar to approach in Francois Chollet's Keras library\n",
    "    if causal:\n",
    "        offset = dilation * (width - 1)\n",
    "        input_seq = tf.pad(input_seq, [[0, 0], [offset, 0], [0, 0]])\n",
    "    \n",
    "    conv_output = tf.nn.convolution(\n",
    "        input=input_seq,\n",
    "        filter=conv_kernel,\n",
    "        padding=\"VALID\" if causal else \"SAME\",\n",
    "        strides=None,\n",
    "        dilation_rate=[dilation]\n",
    "    )\n",
    "    \n",
    "    return conv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.nn.l2_loss(y_predicted - y_seq_ph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = int(1e4)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_evals = []\n",
    "\n",
    "for step in xrange(n_steps):\n",
    "    x_seq = generate_samples(batch_size, sequence_length, 1)\n",
    "    \n",
    "    loss_eval, _ = session.run((loss, optimizer), \n",
    "                               feed_dict={x_seq_ph: x_seq[:,:-1],\n",
    "                                          y_seq_ph: x_seq[:,1:]})\n",
    "\n",
    "    loss_evals.append(loss_eval)\n",
    "    \n",
    "plt.plot(loss_evals)\n",
    "plt.title(\"Squared l2 loss\")\n",
    "plt.xlabel(\"SGD step\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
