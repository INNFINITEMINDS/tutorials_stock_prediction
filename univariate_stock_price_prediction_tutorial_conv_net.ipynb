{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.2\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "d_x = 1\n",
    "d_y = 1\n",
    "\n",
    "target_stock = 'AAPL'\n",
    "stock_list = [target_stock]\n",
    "n_stocks = 1\n",
    "\n",
    "start_date = '20150101'\n",
    "end_date = '20151231'\n",
    "\n",
    "n_ms = 2000\n",
    "sampling_freq = ('%dms' % n_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = '/scratch/belletti/trade_data/'\n",
    "colnames = ['date',\n",
    "            'time',\n",
    "            'ex',\n",
    "            'symbol',\n",
    "            'trade_cond',\n",
    "            'size',\n",
    "            'price',\n",
    "            'stopinf',\n",
    "            'corr',\n",
    "            'seqnum',\n",
    "            'source',\n",
    "            'rf']\n",
    "\n",
    "def read_from_csv(stock, date):\n",
    "    \n",
    "    local_file_path = folder_name + ('%s_%s.csv.gz' % (date, stock))\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            local_file_path,\n",
    "            names=None,\n",
    "            parse_dates={'datetime': ['date', 'time']}, \n",
    "            compression='gzip',\n",
    "            engine='c').set_index('datetime')\n",
    "    except IOError as e:\n",
    "        return None\n",
    "    \n",
    "    return df[['price', 'size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range(start_date, end_date).map(lambda x: ''.join(str(x).split(' ')[0].split('-')))\n",
    "\n",
    "data_set = [read_from_csv(stock, date)\n",
    "            for date in dates\n",
    "            for stock in stock_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dates = [dates[i] for i in xrange(len(dates)) if data_set[i] is not None]\n",
    "data_set = [x for x in data_set if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:00:00.019</th>\n",
       "      <td>111.00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:05:18.110</th>\n",
       "      <td>110.97</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:09:03.440</th>\n",
       "      <td>110.97</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:35:11.488</th>\n",
       "      <td>111.03</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:37:56.331</th>\n",
       "      <td>111.05</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:37:56.331</th>\n",
       "      <td>111.05</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:39:30.647</th>\n",
       "      <td>111.00</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:41:16.702</th>\n",
       "      <td>111.00</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:44:28.623</th>\n",
       "      <td>110.95</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:44:28.623</th>\n",
       "      <td>110.95</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:53:03.787</th>\n",
       "      <td>111.10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:59:53.540</th>\n",
       "      <td>111.19</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:00:24.441</th>\n",
       "      <td>111.19</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:00:24.441</th>\n",
       "      <td>111.20</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:00:24.521</th>\n",
       "      <td>111.20</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:00:30.509</th>\n",
       "      <td>111.20</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:00:46.780</th>\n",
       "      <td>111.20</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:00:55.826</th>\n",
       "      <td>111.20</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:01:36.943</th>\n",
       "      <td>111.00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:05:07.294</th>\n",
       "      <td>110.90</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:18:26.975</th>\n",
       "      <td>111.14</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:18:26.975</th>\n",
       "      <td>111.08</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:21:20.354</th>\n",
       "      <td>111.01</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:21:20.502</th>\n",
       "      <td>110.99</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:22:49.372</th>\n",
       "      <td>111.01</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:22:49.372</th>\n",
       "      <td>111.01</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:23:38.932</th>\n",
       "      <td>111.10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:23:42.877</th>\n",
       "      <td>111.01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:27:18.473</th>\n",
       "      <td>111.10</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 05:33:39.867</th>\n",
       "      <td>111.10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:23:10.132</th>\n",
       "      <td>109.06</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:25:59.876</th>\n",
       "      <td>109.07</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:28:32.779</th>\n",
       "      <td>109.07</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:28:32.779</th>\n",
       "      <td>109.07</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:28:33.301</th>\n",
       "      <td>109.15</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:28:33.307</th>\n",
       "      <td>109.15</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:32:38.933</th>\n",
       "      <td>109.07</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:33:31.178</th>\n",
       "      <td>109.15</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:33:31.178</th>\n",
       "      <td>109.15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:33:31.181</th>\n",
       "      <td>109.17</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:33:31.183</th>\n",
       "      <td>109.16</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:33:31.183</th>\n",
       "      <td>109.13</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:34:51.997</th>\n",
       "      <td>109.18</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:34:52.002</th>\n",
       "      <td>109.18</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:35:39.062</th>\n",
       "      <td>109.13</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:35:39.065</th>\n",
       "      <td>109.16</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:35:43.941</th>\n",
       "      <td>109.07</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:44:54.854</th>\n",
       "      <td>109.17</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:44:54.943</th>\n",
       "      <td>109.18</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:46:53.004</th>\n",
       "      <td>109.15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:49:53.935</th>\n",
       "      <td>109.11</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:49:53.935</th>\n",
       "      <td>109.07</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:49:53.938</th>\n",
       "      <td>109.07</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:49:53.938</th>\n",
       "      <td>109.07</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:53:00.932</th>\n",
       "      <td>109.07</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:54:21.183</th>\n",
       "      <td>109.07</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:57:38.243</th>\n",
       "      <td>109.10</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:58:41.856</th>\n",
       "      <td>109.13</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:58:51.145</th>\n",
       "      <td>109.12</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 19:59:15.302</th>\n",
       "      <td>109.08</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287741 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          price  size\n",
       "datetime                             \n",
       "2015-01-02 04:00:00.019  111.00   100\n",
       "2015-01-02 04:05:18.110  110.97   300\n",
       "2015-01-02 04:09:03.440  110.97    40\n",
       "2015-01-02 04:35:11.488  111.03   200\n",
       "2015-01-02 04:37:56.331  111.05   106\n",
       "2015-01-02 04:37:56.331  111.05    53\n",
       "2015-01-02 04:39:30.647  111.00   800\n",
       "2015-01-02 04:41:16.702  111.00  1200\n",
       "2015-01-02 04:44:28.623  110.95   100\n",
       "2015-01-02 04:44:28.623  110.95    75\n",
       "2015-01-02 04:53:03.787  111.10   100\n",
       "2015-01-02 04:59:53.540  111.19    80\n",
       "2015-01-02 05:00:24.441  111.19   120\n",
       "2015-01-02 05:00:24.441  111.20    10\n",
       "2015-01-02 05:00:24.521  111.20   500\n",
       "2015-01-02 05:00:30.509  111.20   500\n",
       "2015-01-02 05:00:46.780  111.20   800\n",
       "2015-01-02 05:00:55.826  111.20    70\n",
       "2015-01-02 05:01:36.943  111.00   100\n",
       "2015-01-02 05:05:07.294  110.90   430\n",
       "2015-01-02 05:18:26.975  111.14    68\n",
       "2015-01-02 05:18:26.975  111.08   100\n",
       "2015-01-02 05:21:20.354  111.01   200\n",
       "2015-01-02 05:21:20.502  110.99   130\n",
       "2015-01-02 05:22:49.372  111.01    70\n",
       "2015-01-02 05:22:49.372  111.01    30\n",
       "2015-01-02 05:23:38.932  111.10    13\n",
       "2015-01-02 05:23:42.877  111.01   100\n",
       "2015-01-02 05:27:18.473  111.10    87\n",
       "2015-01-02 05:33:39.867  111.10    13\n",
       "...                         ...   ...\n",
       "2015-01-02 19:23:10.132  109.06  1000\n",
       "2015-01-02 19:25:59.876  109.07   100\n",
       "2015-01-02 19:28:32.779  109.07   300\n",
       "2015-01-02 19:28:32.779  109.07   300\n",
       "2015-01-02 19:28:33.301  109.15    50\n",
       "2015-01-02 19:28:33.307  109.15    50\n",
       "2015-01-02 19:32:38.933  109.07    50\n",
       "2015-01-02 19:33:31.178  109.15    50\n",
       "2015-01-02 19:33:31.178  109.15   100\n",
       "2015-01-02 19:33:31.181  109.17   125\n",
       "2015-01-02 19:33:31.183  109.16   486\n",
       "2015-01-02 19:33:31.183  109.13   100\n",
       "2015-01-02 19:34:51.997  109.18   100\n",
       "2015-01-02 19:34:52.002  109.18   100\n",
       "2015-01-02 19:35:39.062  109.13   300\n",
       "2015-01-02 19:35:39.065  109.16   100\n",
       "2015-01-02 19:35:43.941  109.07   100\n",
       "2015-01-02 19:44:54.854  109.17    50\n",
       "2015-01-02 19:44:54.943  109.18    25\n",
       "2015-01-02 19:46:53.004  109.15   100\n",
       "2015-01-02 19:49:53.935  109.11   100\n",
       "2015-01-02 19:49:53.935  109.07   150\n",
       "2015-01-02 19:49:53.938  109.07   100\n",
       "2015-01-02 19:49:53.938  109.07    50\n",
       "2015-01-02 19:53:00.932  109.07    72\n",
       "2015-01-02 19:54:21.183  109.07    10\n",
       "2015-01-02 19:57:38.243  109.10  1000\n",
       "2015-01-02 19:58:41.856  109.13   500\n",
       "2015-01-02 19:58:51.145  109.12    35\n",
       "2015-01-02 19:59:15.302  109.08    50\n",
       "\n",
       "[287741 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers_trades(time_series, window=1000):\n",
    "    \n",
    "    time_series['size_upper_bound'] = pd.rolling_quantile(time_series['size'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.99,\n",
    "                                                     min_periods=1)\n",
    "    time_series['size_lower_bound'] = pd.rolling_quantile(time_series['size'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.01,\n",
    "                                                     min_periods=1)\n",
    "    time_series = time_series[time_series['size'] < time_series['size_upper_bound']]\n",
    "    time_series = time_series[time_series['size'] > time_series['size_lower_bound']]\n",
    "\n",
    "    time_series.drop(['size_upper_bound', 'size_lower_bound'], axis = 1)\n",
    "\n",
    "    time_series['price_upper_bound'] = pd.rolling_quantile(time_series['price'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.99,\n",
    "                                                     min_periods=1)\n",
    "    time_series['price_lower_bound'] = pd.rolling_quantile(time_series['price'],\n",
    "                                                     window=window,\n",
    "                                                     quantile=0.01,\n",
    "                                                     min_periods=1)\n",
    "    time_series = time_series[time_series['price'] < time_series['price_upper_bound']]\n",
    "    time_series = time_series[time_series['price'] > time_series['price_lower_bound']]\n",
    "\n",
    "    time_series = time_series.drop(['price_upper_bound', 'price_lower_bound'], axis = 1)\n",
    "\n",
    "    price_median = time_series['price'].median()\n",
    "\n",
    "    time_series = time_series[time_series['price'] > price_median * 0.80]\n",
    "    time_series = time_series[time_series['price'] < price_median * 1.20]\n",
    "\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resample_trades(date, df):\n",
    "    \n",
    "    start_timestamp = pd.to_datetime(date + 'T10:00:00')\n",
    "    end_timestamp = pd.to_datetime(date + 'T14:00:00')\n",
    "        \n",
    "    df = df.resample(sampling_freq).median().fillna(method='pad')    \n",
    "    complete_result = df[start_timestamp:end_timestamp]['price'].diff()[1:]\n",
    "    \n",
    "    n_samples = len(complete_result)\n",
    "    \n",
    "    return complete_result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/belletti/tensorflow/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:6: FutureWarning: pd.rolling_quantile is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(min_periods=1,window=1000,center=False).quantile(quantile=0.99)\n",
      "  \n",
      "/scratch/belletti/tensorflow/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:10: FutureWarning: pd.rolling_quantile is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(min_periods=1,window=1000,center=False).quantile(quantile=0.01)\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/scratch/belletti/tensorflow/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:19: FutureWarning: pd.rolling_quantile is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(min_periods=1,window=1000,center=False).quantile(quantile=0.99)\n",
      "/scratch/belletti/tensorflow/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:23: FutureWarning: pd.rolling_quantile is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(min_periods=1,window=1000,center=False).quantile(quantile=0.01)\n"
     ]
    }
   ],
   "source": [
    "resampled_clean_data_set = [resample_trades(date, remove_outliers_trades(intraday)) \n",
    "                            for (date, intraday) in zip(valid_dates, data_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timestamps_expected = 4 * 3600 * 1000 / n_ms\n",
    "data_set_array = np.asanyarray([x \n",
    "                                for x in resampled_clean_data_set\n",
    "                                if len(x) == n_timestamps_expected])\n",
    "print(data_set_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize a couple of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days_to_plot = 4\n",
    "\n",
    "for day in xrange(n_days_to_plot):\n",
    "    plt.plot(data_set_array[np.random.randint(data_set_array.shape[0])])\n",
    "    plt.title('Intraday return series for %s between 10AM and 2PM' % target_stock)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We do not assume we know the sequence length yet\n",
    "\n",
    "x_seq_ph = tf.placeholder(shape=(batch_size, None), dtype=tf.float32)\n",
    "x_seq = tf.expand_dims(x_seq_ph, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_layer(input_seq, n_dims_in, n_dims_out, width, \n",
    "                      dilation=1, causal=True):\n",
    "    conv_kernel = tf.get_variable(\n",
    "        name='kernel',\n",
    "        shape=[width, n_dims_in, n_dims_out], \n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    # Similar to approach in Francois Chollet's Keras library\n",
    "    if causal:\n",
    "        offset = dilation * (width - 1)\n",
    "        input_seq = tf.pad(input_seq, [[0, 0], [offset, 0], [0, 0]])\n",
    "    \n",
    "    conv_output = tf.nn.convolution(\n",
    "        input=input_seq,\n",
    "        filter=conv_kernel,\n",
    "        padding='VALID' if causal else 'SAME',\n",
    "        strides=None,\n",
    "        dilation_rate=[dilation]\n",
    "    )\n",
    "    \n",
    "    bias = tf.get_variable(\n",
    "        name='bias',\n",
    "        shape=[n_dims_out],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer()\n",
    "    )\n",
    "    \n",
    "    return tf.nn.bias_add(conv_output, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, leakiness=0.1):\n",
    "    \"\"\"Relu, with optional leaky support.\"\"\"\n",
    "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_conv_net(input_seq, kernel_specs, dilations=None, causal=True):\n",
    "\n",
    "    if dilations is None:\n",
    "        dilations = [1] * len(kernel_specs)\n",
    "    \n",
    "    output_seq = input_seq\n",
    "    for i, ((n_dims_in, n_dims_out, width), dilation) in enumerate(zip(kernel_specs, dilations)):\n",
    "        \n",
    "        with tf.variable_scope('conv_layer_%d' % i):\n",
    "            output_seq = convolution_layer(output_seq, n_dims_in, n_dims_out, \n",
    "                                           dilation, causal)        \n",
    "            output_seq = leaky_relu(output_seq)\n",
    "        \n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_specs = [\n",
    "    [1, 2, 4],\n",
    "    [2, 2, 4],\n",
    "    [2, 2, 4],\n",
    "    [2, 1, 4]  \n",
    "]\n",
    "\n",
    "predicted = build_conv_net(x_seq[:,:-1,:], kernel_specs, dilations=[1,2,4,8])\n",
    "\n",
    "to_predict =  x_seq[:,1:,:]\n",
    "\n",
    "#loss = tf.reduce_sum(tf.nn.l2_loss(predicted - to_predict))\n",
    "loss = tf.reduce_mean(tf.abs(predicted - to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_days = data_set_array.shape[0]\n",
    "sequence_length = 4096\n",
    "train_test_split = 0.9\n",
    "n_days_train = int(n_days * train_test_split)\n",
    "\n",
    "def generate_train_samples(batch_size):\n",
    "    selected_days = np.random.choice(range(n_days_train), batch_size)    \n",
    "    offset = np.random.randint(0, n_timestamps_expected - sequence_length)\n",
    "    return data_set_array[selected_days,offset:]\n",
    "\n",
    "def generate_test_samples(batch_size):\n",
    "    selected_days = np.random.choice(range(n_days_train, n_days), batch_size)    \n",
    "    offset = np.random.randint(0, n_timestamps_expected - sequence_length)\n",
    "    return data_set_array[selected_days,offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = int(2*1e3)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss_evals = []\n",
    "test_loss_evals = []\n",
    "\n",
    "for step in xrange(n_steps):\n",
    "    \n",
    "    x_seq_train_samples = generate_train_samples(batch_size)\n",
    "    train_loss_eval, _ = session.run((loss, optimizer), \n",
    "                                     feed_dict={x_seq_ph: x_seq_train_samples})\n",
    "    train_loss_evals.append(train_loss_eval)\n",
    "    \n",
    "    x_seq_test_samples = generate_test_samples(batch_size)\n",
    "    test_loss_eval, _ = session.run((loss, optimizer), \n",
    "                                    feed_dict={x_seq_ph: x_seq_test_samples})\n",
    "    test_loss_evals.append(test_loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_evals)\n",
    "plt.plot(test_loss_evals)\n",
    "plt.legend(('In sample', 'Out of sample', ))\n",
    "plt.title('l1 loss')\n",
    "plt.xlabel('SGD step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(test_loss_evals[1000:1100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
